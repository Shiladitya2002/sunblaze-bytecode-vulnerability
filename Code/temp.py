from datasets import Dataset
from transformers import AutoModel, AutoConfig, AutoTokenizer
from transformers import DataCollatorWithPadding, TrainingArguments, T5EncoderModel, AutoModelForSeq2SeqLM
from models import TransformerModel, codet5p_embedding
import torch

checkpoint = "Salesforce/codet5p-110m-embedding"
device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)

model = codet5p_embedding(device)

asm = ['mstore callvalue dup1 iszero', 'jumpi dup1 revert pop dup1 codecopy', 'return stop sub_0 mstore']
tokens = model.tokenize(asm)
print(tokens.input_ids.shape)
output = model.forward(tokens)
print(output.shape)

exit()
config = AutoConfig.from_pretrained(checkpoint, trust_remote_code=True)
print(config)
tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)
model = AutoModel.from_pretrained(checkpoint, trust_remote_code=True).to(device)

asm = ['mstore callvalue dup1 iszero', 'jumpi dup1 revert pop dup1 codecopy', 'return stop sub_0 mstore']
tokens = tokenizer(asm, padding=True, return_tensors="pt").to(device)
print(tokens.input_ids)
output = model(tokens.input_ids)
print(output.shape)

d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``
nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``
nhead = 2  # number of heads in ``nn.MultiheadAttention``
dropout = 0.2  # dropout probability
model = TransformerModel(nhead=nhead, d_hid=d_hid, nlayers=nlayers, dropout=dropout).to(device)
output=output[None, :]
out = model(output)
print(out.shape)

exit()

data = ["dd" for i in range(0, 20)]
ds = Dataset.from_dict({"data": data})
print(ds)

training_args = TrainingArguments(output_dir="test_trainer")

checkpoint = "Salesforce/codet5p-770m-py"
device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)

tokenizer = AutoTokenizer.from_pretrained(checkpoint, add_prefix_space=True)
model = T5EncoderModel.from_pretrained(checkpoint).to(device)

triplet_loss = torch.nn.TripletMarginLoss(margin=1.0, p=2)
optimizer = torch.optim.SGD(
    model.parameters(),
    lr = 0.001
)

arr = ["def print_hello_world():", "**dDdjd3j eeejefjef", "System.out.println(Hello World)"]
inputs = tokenizer.encode(text=arr, return_tensors="pt", is_split_into_words=True).to(device)
print(inputs.shape)
output = model(inputs).last_hidden_state
print(output)
output1, output2, output3 = output[0], output[1], output[2]

'''inputs1 = tokenizer.encode("def print_hello_world():", return_tensors="pt").to(device)
output1 = model(inputs1).last_hidden_state
inputs2 = tokenizer.encode("**dDdjd3j eeejefjef", return_tensors="pt").to(device)
output2 = model(inputs2).last_hidden_state
inputs3 = tokenizer.encode("System.out.println(Hello World)", return_tensors="pt").to(device)
output3 = model(inputs3).last_hidden_state'''
triplet_loss = torch.nn.TripletMarginLoss()
output1 = torch.mean(output1, dim=1)
output2 = torch.mean(output2, dim=1)
output3 = torch.mean(output3, dim=1)
print(torch.norm(output1[0] - output2[0]))
print(torch.norm(output1[0] - output3[0]))
loss = triplet_loss(output1[0], output2[0], output3[0])
print(loss)
loss.backward()
print(loss.grad)
for i in range(0, 100):
    optimizer.step()
optimizer.zero_grad()

inputs4 = tokenizer.encode("def print_hello_world():", return_tensors="pt").to(device)
output4 = model(inputs4).last_hidden_state
output4 = torch.mean(output4, dim=1)
print(torch.equal(output4.detach(), output1.detach()))

