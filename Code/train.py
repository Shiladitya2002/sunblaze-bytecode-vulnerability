from transformers import T5EncoderModel, AutoTokenizer, AutoModel, LongT5EncoderModel, get_scheduler
from transformers import LongformerModel, AutoModelForSeq2SeqLM, AutoConfig
from datasets import Dataset, load_dataset
import torch
import time
import random
from pathlib import Path
import json
import os
import numpy as np
from models import codet5p_embedding

#Set Environ Variables
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)

#Training_Params
num_train_epochs = 2
trainval_split = 0.98
max_block = 300
model_dir = Path("C:\\Documents\\Dawn Song Lab\\Explore\\Encode_1\\Models\\Train_1")
data_dir = Path("C:\\Documents\\Dawn Song Lab\\Dataset\\normal-unoptimize_15000\\json_dataset2")

#Helper Methods
def file_loader(data_path):
    for version in os.listdir(data_path):
        for contract in os.listdir(data_path / version):
            for file in os.listdir(data_path / version / contract):
                yield data_path / version / contract / file
            
def json_load(path):
    rf = open(str(path), "r", encoding='utf-8')
    return json.load(rf)

def batcher(iterable, n=1):
    l = len(iterable)
    for ndx in range(0, l, n):
        yield iterable[ndx:min(ndx + n, l)]

def forward(asm):
    tokens = model.tokenize(asm)
    print(tokens.input_ids.shape)
    output = model.forward(tokens)
    return output

def evaluate(val_paths):
    losses = []
    for contract_path in val_paths:
        # Load JSON
        contract_pair = json_load(contract_path)
        misc_pair = json_load(random.choice(val_paths))
        # Get Encoding
        with torch.no_grad():
            try:
                opt_logits = model.tokenize_forward(contract_pair['opt'])
                unopt_logits = model.tokenize_forward(contract_pair['unopt'])
                misc_logits = model.tokenize_forward(misc_pair[random.choice(['opt', 'unopt'])])
            except RuntimeError as e:
                torch.cuda.empty_cache()
                continue
            # Get Loss
            losses.append(triplet_loss(opt_logits, unopt_logits, misc_logits).item())
    return np.mean(losses)

#Initialize Model
model = codet5p_embedding(device)
print(model.config)

#Load Data and Split
data_paths = [file for file in file_loader(data_dir)]
train_paths = data_paths[:int(trainval_split*len(data_paths))]
val_paths = data_paths[int(trainval_split*len(data_paths)):]

#Setup Optimizers
num_update_steps_per_epoch = len(train_paths)
num_training_steps = num_train_epochs * num_update_steps_per_epoch
triplet_loss = torch.nn.TripletMarginLoss(margin=1.0, p=2)
optimizer = torch.optim.SGD(
    list(model.encode_model.parameters()) + list(model.model.parameters()),
    lr=0.001,) 
#    betas=(0.9, 0.999), 
#    eps=1e-08
#)
#lr_scheduler = get_scheduler(
#    name="linear",
#    optimizer=optimizer,
#    num_warmup_steps=1_000,
#    num_training_steps=num_training_steps,
#)
lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer=optimizer, 
    T_max=num_training_steps)
model.save(model_dir / str(1))
# Run training loop
steps = 0
stime = time.time()
for epoch in range(num_train_epochs):
    random.shuffle(train_paths)
    for contract_path in train_paths:
        # Load JSON
        contract_pair = json_load(contract_path)
        misc_pair = json_load(random.choice(train_paths))
        if len(contract_pair['opt'])>max_block or len(misc_pair['opt'])>max_block:
            print("SKIP: TOO MANY BLOCKS")
            continue
        # Get Encoding
        try:
            opt_logits = model.tokenize_forward(contract_pair['opt'])
            unopt_logits = model.tokenize_forward(contract_pair['unopt'])
            misc_logits = model.tokenize_forward(misc_pair[random.choice(['opt', 'unopt'])])
        except RuntimeError as e:
            print("ERROR: CUDA OUT OF MEMORY")
            optimizer.zero_grad()
            torch.cuda.empty_cache()
            continue
        print('PASS DONE')
        # Get Loss
        loss = triplet_loss(opt_logits, unopt_logits, misc_logits)
        # Step
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        torch.cuda.empty_cache()
        # Update Progress
        steps += 1
        if steps%1000==0:
            etime = time.time()
            print('Steps: {step} | Time: {time}'.format(step=steps, time = str(etime-stime)))
            stime = time.time()
        if steps%10000==0: 
            print('Steps: {step} | Loss: {loss}'.format(step=steps, loss=evaluate(val_paths)))
        if steps%15000==0:
            print('Saving Model')
            model.save(model_dir / str(steps))
    print('-----------------------------------------')
    print('Completed Epoch: {epoch}'.format(epoch=epoch))
print("Finished Training")
model.save(model_dir / "final")